{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploads=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "cYhHUvAt9Y4v",
        "outputId": "c0ad96a2-8c29-4973-dd0e-6d2d05098071"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3139d30a-e747-4297-b397-e0552cfc67c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3139d30a-e747-4297-b397-e0552cfc67c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CSE508_Winter2023_Dataset.zip to CSE508_Winter2023_Dataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import math\n",
        "os.mkdir('temp_dir')          #creating a new directory called temp_dir\n",
        "from zipfile import ZipFile\n",
        "with ZipFile(\"CSE508_Winter2023_Dataset.zip\", 'r') as zObject:  \n",
        "    zObject.extractall(path=\"temp_dir\")                           # Extracting all the members of the zip into directory temp_dir"
      ],
      "metadata": {
        "id": "tsaqPMoCm43b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for Relevant Text Extraction from the document files and then saving those content to the same file\n",
        "def extract_text(fname):\n",
        "  fobj=open(fname,'r')\n",
        "  contents=fobj.read()\n",
        "  sp=BeautifulSoup(contents,'lxml') \n",
        "  text_txt=''\n",
        "  title_txt=''\n",
        "  for txt in sp.find_all('text'):             # extracting the contents between the <TEXT>...</TEXT> tags\n",
        "    text_txt=txt.get_text()\n",
        "  for title in sp.find_all('title'):          # extracting the contents between the <TITLE>...</TITLE> tags\n",
        "    title_txt=title.get_text()\n",
        "  fin_txt=''.join([title_txt,text_txt])       # concatenating the 2 strings text_txt and title_txt using blank space.\n",
        "  fobj.close()\n",
        "  f_wrt=open(fname,'w')\n",
        "  f_wrt.write(fin_txt)\n",
        "  f_wrt.close()\n"
      ],
      "metadata": {
        "id": "YUNNuiqPoTsv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetching each files of CSE508_Winter2023_Dataset from temp_dir directory and doing Relevant Text Extraction for all documents\n",
        "\n",
        "files_list=os.listdir('/content/temp_dir/CSE508_Winter2023_Dataset')\n",
        "dir_path='/content/temp_dir/CSE508_Winter2023_Dataset'\n",
        "for fl in files_list:\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  extract_text(fl_path)                                "
      ],
      "metadata": {
        "id": "n3L738hcEM8E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Preprocessing"
      ],
      "metadata": {
        "id": "Iw5_fJ96uQ48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing the content of first 5 files before any preprocessing step"
      ],
      "metadata": {
        "id": "kTefWiXLbdqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 1\n",
        "for fl in files_list:\n",
        "  print(\"************************ File_{} ************************\".format(count))\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r')\n",
        "  text=fobj.read()\n",
        "  print(text)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  fobj.close()\n",
        "  count +=1\n",
        "  if count > 5:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDBMP7eZuBs8",
        "outputId": "17919dcc-39a5-4d58-b922-e5bd9840bf1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************ File_1 ************************\n",
            "\n",
            "transition measurements on cones in free flight ballistics\n",
            "range tests .\n",
            "\n",
            "  navy-sponsored experimental investigation of the location of\n",
            "boundary-layer transition on sharp-nosed cones having 10 total\n",
            "angles .  the ambient temperature in a portion of the aeroballistics\n",
            "range is varied so as to obtain different adiabatic recovery temperatures\n",
            "at a constant nominal mach number of 3.1 .  the location\n",
            "of transition is expressed as a transition reynolds number, and\n",
            "results are presented graphically as a function of the ratio between\n",
            "the wall temperature and the adiabatic recovery temperature .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_2 ************************\n",
            "\n",
            "dynamic stability of vehicles traversing ascending\n",
            "or descending paths through the atmosphere .\n",
            "\n",
            "  an analysis is given of the oscillatory motions of vehicles which\n",
            "traverse ascending and descending paths through the atmosphere at high\n",
            "speed .  the specific case of a skip path is examined in detail, and\n",
            "this leads to a form of solution for the oscillatory motion which should\n",
            "recur over any trajectory .  the distinguishing feature of this form is\n",
            "the appearance of the bessel rather than the trigonometric function as\n",
            "the characteristic mode of oscillation .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_3 ************************\n",
            "\n",
            "performance estimates for the rae 6in . high-pressure\n",
            "shock tube .\n",
            "\n",
            "  estimates are made of the performance\n",
            "of the rae 6'' high pressure\n",
            "shock tube, with various driver gases, over\n",
            "a range of pressure ratios giving\n",
            "shock mach numbers from 6 to 22 .  the\n",
            "calculations are based on a simplified\n",
            "model of shock tube flow, in which the\n",
            "working fluid (argon-free air) is\n",
            "assumed to be always in chemical\n",
            "equilibrium, and the driver gas (either\n",
            "hydrogen, or the products of combustion\n",
            "of a hydrogen-oxygen mixture) is\n",
            "assumed to behave as an ideal gas with\n",
            "constant specifiic heats .\n",
            "  the results are presented in\n",
            "graphical form and comprise charts\n",
            "normal shock waves in argon-free air\n",
            "shock wave mach number and diaphragm\n",
            "pressure ratio under various initial conditions, and\n",
            "of the shock-induced flows, both\n",
            "in the uniform-sectioned shock tube, and\n",
            "when expanded in a divergent nozzle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_4 ************************\n",
            "\n",
            "calculated responses of a large sweptwing airplane\n",
            "to continuous turbulence with flight-test comparisons .\n",
            "\n",
            "  calculated responses of symmetrical\n",
            "airplane motions, wing deformations,\n",
            "and wing loads due to gusts are\n",
            "shown to compare favorably with\n",
            "available flight-test results .  these\n",
            "calculated responses are based on\n",
            "random-process theory, five degrees of\n",
            "freedom, lifting-surface aerodynamics,\n",
            "and one-dimensional vertical\n",
            "turbulence .  the extent to which\n",
            "various degrees of freedom contribute\n",
            "to the responses is examined and\n",
            "in this connection the relative effects\n",
            "of static and dynamic aeroelasticity\n",
            "are determined .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_5 ************************\n",
            "\n",
            "principles of creep buckling weight-strength analysis\n",
            "of aircraft structures .\n",
            "\n",
            "  the possibility of a gradual instability\n",
            "failure of a column under compressive\n",
            "load has been recognized\n",
            "for some time .  marin presented an\n",
            "analysis of creep buckling based on\n",
            "a theory of creep bending, but did not\n",
            "take into account the average stress\n",
            "due to axial loading .  the theory also\n",
            "neglected the transient (nonlinear)\n",
            "portion of the creep curve .\n",
            "  in efficient column design, the\n",
            "average stress should be relatively high in\n",
            "comparison with the bending stresses,.\n",
            "that is, the column should be as straight\n",
            "as possible and the slenderness ratio\n",
            "should not be too great .  under these\n",
            "conditions marin's theory is not\n",
            "directly applicable, although it gives good\n",
            "agreement with tests of columns\n",
            "having large slenderness ratios or large\n",
            "eccentricities .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing the content of first 5 files after lowercasing the file contents"
      ],
      "metadata": {
        "id": "q7xfhf5SbuQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fl in files_list:                 # making update for all files in temp_dir\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r+')\n",
        "  text=fobj.read()                    # fetching the file content into a string called 'text'\n",
        "  fobj.seek(0)                        # absolute file positioning\n",
        "  fobj.truncate()                     # to erase all data \n",
        "  fobj.write(text.lower())            # writing the lowercased content in the same file\n",
        "  fobj.close()\n",
        "\n",
        "\n",
        "count = 1\n",
        "print(\"Sample files after lowercasing the texts !!! \\n\\n\")\n",
        "for fl in files_list:\n",
        "  print(\"************************ File_{} ************************\".format(count))\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r')\n",
        "  text=fobj.read()\n",
        "  print(text)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  fobj.close()\n",
        "  count +=1\n",
        "  if count > 5:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kJh2x9WuB5t",
        "outputId": "3cf87bee-fd22-4589-c1fe-3a9107885183"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files after lowercasing the texts !!! \n",
            "\n",
            "\n",
            "************************ File_1 ************************\n",
            "\n",
            "transition measurements on cones in free flight ballistics\n",
            "range tests .\n",
            "\n",
            "  navy-sponsored experimental investigation of the location of\n",
            "boundary-layer transition on sharp-nosed cones having 10 total\n",
            "angles .  the ambient temperature in a portion of the aeroballistics\n",
            "range is varied so as to obtain different adiabatic recovery temperatures\n",
            "at a constant nominal mach number of 3.1 .  the location\n",
            "of transition is expressed as a transition reynolds number, and\n",
            "results are presented graphically as a function of the ratio between\n",
            "the wall temperature and the adiabatic recovery temperature .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_2 ************************\n",
            "\n",
            "dynamic stability of vehicles traversing ascending\n",
            "or descending paths through the atmosphere .\n",
            "\n",
            "  an analysis is given of the oscillatory motions of vehicles which\n",
            "traverse ascending and descending paths through the atmosphere at high\n",
            "speed .  the specific case of a skip path is examined in detail, and\n",
            "this leads to a form of solution for the oscillatory motion which should\n",
            "recur over any trajectory .  the distinguishing feature of this form is\n",
            "the appearance of the bessel rather than the trigonometric function as\n",
            "the characteristic mode of oscillation .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_3 ************************\n",
            "\n",
            "performance estimates for the rae 6in . high-pressure\n",
            "shock tube .\n",
            "\n",
            "  estimates are made of the performance\n",
            "of the rae 6'' high pressure\n",
            "shock tube, with various driver gases, over\n",
            "a range of pressure ratios giving\n",
            "shock mach numbers from 6 to 22 .  the\n",
            "calculations are based on a simplified\n",
            "model of shock tube flow, in which the\n",
            "working fluid (argon-free air) is\n",
            "assumed to be always in chemical\n",
            "equilibrium, and the driver gas (either\n",
            "hydrogen, or the products of combustion\n",
            "of a hydrogen-oxygen mixture) is\n",
            "assumed to behave as an ideal gas with\n",
            "constant specifiic heats .\n",
            "  the results are presented in\n",
            "graphical form and comprise charts\n",
            "normal shock waves in argon-free air\n",
            "shock wave mach number and diaphragm\n",
            "pressure ratio under various initial conditions, and\n",
            "of the shock-induced flows, both\n",
            "in the uniform-sectioned shock tube, and\n",
            "when expanded in a divergent nozzle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_4 ************************\n",
            "\n",
            "calculated responses of a large sweptwing airplane\n",
            "to continuous turbulence with flight-test comparisons .\n",
            "\n",
            "  calculated responses of symmetrical\n",
            "airplane motions, wing deformations,\n",
            "and wing loads due to gusts are\n",
            "shown to compare favorably with\n",
            "available flight-test results .  these\n",
            "calculated responses are based on\n",
            "random-process theory, five degrees of\n",
            "freedom, lifting-surface aerodynamics,\n",
            "and one-dimensional vertical\n",
            "turbulence .  the extent to which\n",
            "various degrees of freedom contribute\n",
            "to the responses is examined and\n",
            "in this connection the relative effects\n",
            "of static and dynamic aeroelasticity\n",
            "are determined .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_5 ************************\n",
            "\n",
            "principles of creep buckling weight-strength analysis\n",
            "of aircraft structures .\n",
            "\n",
            "  the possibility of a gradual instability\n",
            "failure of a column under compressive\n",
            "load has been recognized\n",
            "for some time .  marin presented an\n",
            "analysis of creep buckling based on\n",
            "a theory of creep bending, but did not\n",
            "take into account the average stress\n",
            "due to axial loading .  the theory also\n",
            "neglected the transient (nonlinear)\n",
            "portion of the creep curve .\n",
            "  in efficient column design, the\n",
            "average stress should be relatively high in\n",
            "comparison with the bending stresses,.\n",
            "that is, the column should be as straight\n",
            "as possible and the slenderness ratio\n",
            "should not be too great .  under these\n",
            "conditions marin's theory is not\n",
            "directly applicable, although it gives good\n",
            "agreement with tests of columns\n",
            "having large slenderness ratios or large\n",
            "eccentricities .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing the content of first 5 files after tokenizing the file contents"
      ],
      "metadata": {
        "id": "jtfyNVE4b29b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "for fl in files_list:                        # tokenizing all the files of temp_dir\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r+')\n",
        "  text=fobj.read()\n",
        "  words=nltk.word_tokenize(text)             # tokenizing the file content and storing those tokens into a list 'words'\n",
        "  new_text = \",\".join(words)                 # joining all those tokens using comma as a seperator and storing them to a string 'new_text' \n",
        "  fobj.seek(0)                               # absolute file positioning\n",
        "  fobj.truncate()                            # to erase all data \n",
        "  fobj.write(new_text)                       # writing 'new_text' to the same file\n",
        "  fobj.close()\n",
        "\n",
        "count = 1\n",
        "print(\"Sample files after tokenizing the file contents !!! \\n\\n\")\n",
        "for fl in files_list:\n",
        "  print(\"************************ File_{} ************************\".format(count))\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r')\n",
        "  text=fobj.read()\n",
        "  print(text)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  fobj.close()\n",
        "  count +=1\n",
        "  if count > 5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8rynR2duB9L",
        "outputId": "4b1066fb-a0cb-462a-e527-11d79427bb3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files after tokenizing the file contents !!! \n",
            "\n",
            "\n",
            "************************ File_1 ************************\n",
            "transition,measurements,on,cones,in,free,flight,ballistics,range,tests,.,navy-sponsored,experimental,investigation,of,the,location,of,boundary-layer,transition,on,sharp-nosed,cones,having,10,total,angles,.,the,ambient,temperature,in,a,portion,of,the,aeroballistics,range,is,varied,so,as,to,obtain,different,adiabatic,recovery,temperatures,at,a,constant,nominal,mach,number,of,3.1,.,the,location,of,transition,is,expressed,as,a,transition,reynolds,number,,,and,results,are,presented,graphically,as,a,function,of,the,ratio,between,the,wall,temperature,and,the,adiabatic,recovery,temperature,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_2 ************************\n",
            "dynamic,stability,of,vehicles,traversing,ascending,or,descending,paths,through,the,atmosphere,.,an,analysis,is,given,of,the,oscillatory,motions,of,vehicles,which,traverse,ascending,and,descending,paths,through,the,atmosphere,at,high,speed,.,the,specific,case,of,a,skip,path,is,examined,in,detail,,,and,this,leads,to,a,form,of,solution,for,the,oscillatory,motion,which,should,recur,over,any,trajectory,.,the,distinguishing,feature,of,this,form,is,the,appearance,of,the,bessel,rather,than,the,trigonometric,function,as,the,characteristic,mode,of,oscillation,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_3 ************************\n",
            "performance,estimates,for,the,rae,6in,.,high-pressure,shock,tube,.,estimates,are,made,of,the,performance,of,the,rae,6,'',high,pressure,shock,tube,,,with,various,driver,gases,,,over,a,range,of,pressure,ratios,giving,shock,mach,numbers,from,6,to,22,.,the,calculations,are,based,on,a,simplified,model,of,shock,tube,flow,,,in,which,the,working,fluid,(,argon-free,air,),is,assumed,to,be,always,in,chemical,equilibrium,,,and,the,driver,gas,(,either,hydrogen,,,or,the,products,of,combustion,of,a,hydrogen-oxygen,mixture,),is,assumed,to,behave,as,an,ideal,gas,with,constant,specifiic,heats,.,the,results,are,presented,in,graphical,form,and,comprise,charts,normal,shock,waves,in,argon-free,air,shock,wave,mach,number,and,diaphragm,pressure,ratio,under,various,initial,conditions,,,and,of,the,shock-induced,flows,,,both,in,the,uniform-sectioned,shock,tube,,,and,when,expanded,in,a,divergent,nozzle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_4 ************************\n",
            "calculated,responses,of,a,large,sweptwing,airplane,to,continuous,turbulence,with,flight-test,comparisons,.,calculated,responses,of,symmetrical,airplane,motions,,,wing,deformations,,,and,wing,loads,due,to,gusts,are,shown,to,compare,favorably,with,available,flight-test,results,.,these,calculated,responses,are,based,on,random-process,theory,,,five,degrees,of,freedom,,,lifting-surface,aerodynamics,,,and,one-dimensional,vertical,turbulence,.,the,extent,to,which,various,degrees,of,freedom,contribute,to,the,responses,is,examined,and,in,this,connection,the,relative,effects,of,static,and,dynamic,aeroelasticity,are,determined,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_5 ************************\n",
            "principles,of,creep,buckling,weight-strength,analysis,of,aircraft,structures,.,the,possibility,of,a,gradual,instability,failure,of,a,column,under,compressive,load,has,been,recognized,for,some,time,.,marin,presented,an,analysis,of,creep,buckling,based,on,a,theory,of,creep,bending,,,but,did,not,take,into,account,the,average,stress,due,to,axial,loading,.,the,theory,also,neglected,the,transient,(,nonlinear,),portion,of,the,creep,curve,.,in,efficient,column,design,,,the,average,stress,should,be,relatively,high,in,comparison,with,the,bending,stresses,,,.,that,is,,,the,column,should,be,as,straight,as,possible,and,the,slenderness,ratio,should,not,be,too,great,.,under,these,conditions,marin,'s,theory,is,not,directly,applicable,,,although,it,gives,good,agreement,with,tests,of,columns,having,large,slenderness,ratios,or,large,eccentricities,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing the content of first 5 files after removing stopwords from the collection of tokens generated in the last step"
      ],
      "metadata": {
        "id": "caRigJhycAOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=list(stopwords.words(\"english\"))                       # getting the list of stop words defined by NLTK library\n",
        "\n",
        "for fl in files_list:                                             # for all files in temp_dir making updates\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r+')\n",
        "  text=fobj.read()\n",
        "  tokens = text.split(\",\")                                        # extracting the comma seperated tokens from the file to a list called 'tokens'\n",
        "  new_tokens = [t for t in tokens if t not in stop_words]         # filtering the tokens which are not stop words into a new list 'new_tokens'\n",
        "  new_text = \",\".join(new_tokens)                                 # again joining those filtered tokens to a string called 'new_text'\n",
        "  fobj.seek(0)                                                    # absolute file positioning\n",
        "  fobj.truncate()                                                 # to erase all data \n",
        "  fobj.write(new_text)                                            # write the 'new_text' to the same file\n",
        "  fobj.close()\n",
        "\n",
        "count = 1\n",
        "print(\"Sample files after removing stop words !!! \\n\\n\")\n",
        "for fl in files_list:\n",
        "  print(\"************************ File_{} ************************\".format(count))\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r')\n",
        "  text=fobj.read()\n",
        "  print(text)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  fobj.close()\n",
        "  count +=1\n",
        "  if count > 5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCRbzutRuCE7",
        "outputId": "a1227776-5447-41ec-eaf4-23da48e531ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files after removing stop words !!! \n",
            "\n",
            "\n",
            "************************ File_1 ************************\n",
            "transition,measurements,cones,free,flight,ballistics,range,tests,.,navy-sponsored,experimental,investigation,location,boundary-layer,transition,sharp-nosed,cones,10,total,angles,.,ambient,temperature,portion,aeroballistics,range,varied,obtain,different,adiabatic,recovery,temperatures,constant,nominal,mach,number,3.1,.,location,transition,expressed,transition,reynolds,number,,,results,presented,graphically,function,ratio,wall,temperature,adiabatic,recovery,temperature,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_2 ************************\n",
            "dynamic,stability,vehicles,traversing,ascending,descending,paths,atmosphere,.,analysis,given,oscillatory,motions,vehicles,traverse,ascending,descending,paths,atmosphere,high,speed,.,specific,case,skip,path,examined,detail,,,leads,form,solution,oscillatory,motion,recur,trajectory,.,distinguishing,feature,form,appearance,bessel,rather,trigonometric,function,characteristic,mode,oscillation,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_3 ************************\n",
            "performance,estimates,rae,6in,.,high-pressure,shock,tube,.,estimates,made,performance,rae,6,'',high,pressure,shock,tube,,,various,driver,gases,,,range,pressure,ratios,giving,shock,mach,numbers,6,22,.,calculations,based,simplified,model,shock,tube,flow,,,working,fluid,(,argon-free,air,),assumed,always,chemical,equilibrium,,,driver,gas,(,either,hydrogen,,,products,combustion,hydrogen-oxygen,mixture,),assumed,behave,ideal,gas,constant,specifiic,heats,.,results,presented,graphical,form,comprise,charts,normal,shock,waves,argon-free,air,shock,wave,mach,number,diaphragm,pressure,ratio,various,initial,conditions,,,shock-induced,flows,,,uniform-sectioned,shock,tube,,,expanded,divergent,nozzle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_4 ************************\n",
            "calculated,responses,large,sweptwing,airplane,continuous,turbulence,flight-test,comparisons,.,calculated,responses,symmetrical,airplane,motions,,,wing,deformations,,,wing,loads,due,gusts,shown,compare,favorably,available,flight-test,results,.,calculated,responses,based,random-process,theory,,,five,degrees,freedom,,,lifting-surface,aerodynamics,,,one-dimensional,vertical,turbulence,.,extent,various,degrees,freedom,contribute,responses,examined,connection,relative,effects,static,dynamic,aeroelasticity,determined,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_5 ************************\n",
            "principles,creep,buckling,weight-strength,analysis,aircraft,structures,.,possibility,gradual,instability,failure,column,compressive,load,recognized,time,.,marin,presented,analysis,creep,buckling,based,theory,creep,bending,,,take,account,average,stress,due,axial,loading,.,theory,also,neglected,transient,(,nonlinear,),portion,creep,curve,.,efficient,column,design,,,average,stress,relatively,high,comparison,bending,stresses,,,.,,,column,straight,possible,slenderness,ratio,great,.,conditions,marin,'s,theory,directly,applicable,,,although,gives,good,agreement,tests,columns,large,slenderness,ratios,large,eccentricities,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing the content of first 5 files after removing punctuation and special character tokens"
      ],
      "metadata": {
        "id": "Yqbo-lykcS_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "for fl in files_list:\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r+')\n",
        "  text=fobj.read()                                       # obtaining the file content in a string called 'text'\n",
        "  punct = \"[^\\w\\s]\"                                      # regular expression to recognize all characters which are not a word/ number/ blank-space\n",
        "  new_text1 = re.sub(\"^\"+punct+\",\",'',text)              # removing punctuation or sp-char tokens occuring at begining of string\n",
        "  new_text2 = re.sub(\",\"+punct+\",\",', ,',new_text1)      # removing punctuation or sp-char tokens occuring at middle of string\n",
        "  new_text = re.sub(\",\"+punct+\"$\",'',new_text2)          # removing punctuation or sp-char tokens occuring at the extreme end of string\n",
        "  fobj.seek(0)                                           # absolute file positioning\n",
        "  fobj.truncate()                                        # to erase all data \n",
        "  fobj.write(new_text)                                   # writing back the final updated string 'new_text' to the same file\n",
        "  fobj.close()\n",
        "\n",
        "count = 1\n",
        "print(\"Sample files after removing punctuations !!! \\n\\n\")\n",
        "for fl in files_list:\n",
        "  print(\"************************ File_{} ************************\".format(count))\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r')\n",
        "  text=fobj.read()\n",
        "  print(text)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  fobj.close()\n",
        "  count +=1\n",
        "  if count > 5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8IChDA9zMYR",
        "outputId": "35d23c88-5527-441a-b59d-46a652b6ae70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files after removing punctuations !!! \n",
            "\n",
            "\n",
            "************************ File_1 ************************\n",
            "transition,measurements,cones,free,flight,ballistics,range,tests, ,navy-sponsored,experimental,investigation,location,boundary-layer,transition,sharp-nosed,cones,10,total,angles, ,ambient,temperature,portion,aeroballistics,range,varied,obtain,different,adiabatic,recovery,temperatures,constant,nominal,mach,number,3.1, ,location,transition,expressed,transition,reynolds,number, ,results,presented,graphically,function,ratio,wall,temperature,adiabatic,recovery,temperature\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_2 ************************\n",
            "dynamic,stability,vehicles,traversing,ascending,descending,paths,atmosphere, ,analysis,given,oscillatory,motions,vehicles,traverse,ascending,descending,paths,atmosphere,high,speed, ,specific,case,skip,path,examined,detail, ,leads,form,solution,oscillatory,motion,recur,trajectory, ,distinguishing,feature,form,appearance,bessel,rather,trigonometric,function,characteristic,mode,oscillation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_3 ************************\n",
            "performance,estimates,rae,6in, ,high-pressure,shock,tube, ,estimates,made,performance,rae,6,'',high,pressure,shock,tube, ,various,driver,gases, ,range,pressure,ratios,giving,shock,mach,numbers,6,22, ,calculations,based,simplified,model,shock,tube,flow, ,working,fluid, ,argon-free,air, ,assumed,always,chemical,equilibrium, ,driver,gas, ,either,hydrogen, ,products,combustion,hydrogen-oxygen,mixture, ,assumed,behave,ideal,gas,constant,specifiic,heats, ,results,presented,graphical,form,comprise,charts,normal,shock,waves,argon-free,air,shock,wave,mach,number,diaphragm,pressure,ratio,various,initial,conditions, ,shock-induced,flows, ,uniform-sectioned,shock,tube, ,expanded,divergent,nozzle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_4 ************************\n",
            "calculated,responses,large,sweptwing,airplane,continuous,turbulence,flight-test,comparisons, ,calculated,responses,symmetrical,airplane,motions, ,wing,deformations, ,wing,loads,due,gusts,shown,compare,favorably,available,flight-test,results, ,calculated,responses,based,random-process,theory, ,five,degrees,freedom, ,lifting-surface,aerodynamics, ,one-dimensional,vertical,turbulence, ,extent,various,degrees,freedom,contribute,responses,examined,connection,relative,effects,static,dynamic,aeroelasticity,determined\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_5 ************************\n",
            "principles,creep,buckling,weight-strength,analysis,aircraft,structures, ,possibility,gradual,instability,failure,column,compressive,load,recognized,time, ,marin,presented,analysis,creep,buckling,based,theory,creep,bending, ,take,account,average,stress,due,axial,loading, ,theory,also,neglected,transient, ,nonlinear, ,portion,creep,curve, ,efficient,column,design, ,average,stress,relatively,high,comparison,bending,stresses, ,., ,column,straight,possible,slenderness,ratio,great, ,conditions,marin,'s,theory,directly,applicable, ,although,gives,good,agreement,tests,columns,large,slenderness,ratios,large,eccentricities\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing the content of first 5 files after removing blank space tokens"
      ],
      "metadata": {
        "id": "QgxLkOMfep8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fl in files_list:                             # for all files of temp_dir making updates\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r+')\n",
        "  text=fobj.read()                                # fetcing the file content\n",
        "  tokens = text.split(\",\")                        # exctracting the comma seperated tokens into a list called 'tokens'\n",
        "  words = [t for t in tokens if t!=' ']           # filtering the non-blank-space tokens into a new list called 'words'\n",
        "  new_text = \",\".join(words)                      # joining those tokens from the list 'words' into a string called 'new_text'\n",
        "  fobj.seek(0)                                    # absolute file positioning\n",
        "  fobj.truncate()                                 # to erase all data \n",
        "  fobj.write(new_text)                            # writing the string 'new_text' into the same file\n",
        "  fobj.close()\n",
        "\n",
        "count = 1\n",
        "print(\"Sample files after removing blank space tokens !!! \\n\\n\")\n",
        "for fl in files_list:\n",
        "  print(\"************************ File_{} ************************\".format(count))\n",
        "  fl_path=os.path.join(dir_path,fl)\n",
        "  fobj=open(fl_path,'r')\n",
        "  text=fobj.read()\n",
        "  print(text)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  fobj.close()\n",
        "  count +=1\n",
        "  if count > 5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UyajG4ruCOP",
        "outputId": "751d8d16-dc88-4a45-bb8d-6fc83941a351"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files after removing blank space tokens !!! \n",
            "\n",
            "\n",
            "************************ File_1 ************************\n",
            "transition,measurements,cones,free,flight,ballistics,range,tests,navy-sponsored,experimental,investigation,location,boundary-layer,transition,sharp-nosed,cones,10,total,angles,ambient,temperature,portion,aeroballistics,range,varied,obtain,different,adiabatic,recovery,temperatures,constant,nominal,mach,number,3.1,location,transition,expressed,transition,reynolds,number,results,presented,graphically,function,ratio,wall,temperature,adiabatic,recovery,temperature\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_2 ************************\n",
            "dynamic,stability,vehicles,traversing,ascending,descending,paths,atmosphere,analysis,given,oscillatory,motions,vehicles,traverse,ascending,descending,paths,atmosphere,high,speed,specific,case,skip,path,examined,detail,leads,form,solution,oscillatory,motion,recur,trajectory,distinguishing,feature,form,appearance,bessel,rather,trigonometric,function,characteristic,mode,oscillation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_3 ************************\n",
            "performance,estimates,rae,6in,high-pressure,shock,tube,estimates,made,performance,rae,6,'',high,pressure,shock,tube,various,driver,gases,range,pressure,ratios,giving,shock,mach,numbers,6,22,calculations,based,simplified,model,shock,tube,flow,working,fluid,argon-free,air,assumed,always,chemical,equilibrium,driver,gas,either,hydrogen,products,combustion,hydrogen-oxygen,mixture,assumed,behave,ideal,gas,constant,specifiic,heats,results,presented,graphical,form,comprise,charts,normal,shock,waves,argon-free,air,shock,wave,mach,number,diaphragm,pressure,ratio,various,initial,conditions,shock-induced,flows,uniform-sectioned,shock,tube,expanded,divergent,nozzle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_4 ************************\n",
            "calculated,responses,large,sweptwing,airplane,continuous,turbulence,flight-test,comparisons,calculated,responses,symmetrical,airplane,motions,wing,deformations,wing,loads,due,gusts,shown,compare,favorably,available,flight-test,results,calculated,responses,based,random-process,theory,five,degrees,freedom,lifting-surface,aerodynamics,one-dimensional,vertical,turbulence,extent,various,degrees,freedom,contribute,responses,examined,connection,relative,effects,static,dynamic,aeroelasticity,determined\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "************************ File_5 ************************\n",
            "principles,creep,buckling,weight-strength,analysis,aircraft,structures,possibility,gradual,instability,failure,column,compressive,load,recognized,time,marin,presented,analysis,creep,buckling,based,theory,creep,bending,take,account,average,stress,due,axial,loading,theory,also,neglected,transient,nonlinear,portion,creep,curve,efficient,column,design,average,stress,relatively,high,comparison,bending,stresses,.,column,straight,possible,slenderness,ratio,great,conditions,marin,'s,theory,directly,applicable,although,gives,good,agreement,tests,columns,large,slenderness,ratios,large,eccentricities\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after Text Preprocessing of all documents from temp_dir, collecting them to a zipfile CSE508_Winter2023_Dataset_3.zip\n",
        "# then downloading the zipped file to store this dataset locally\n",
        "\n",
        "files_list=os.listdir('/content/temp_dir/CSE508_Winter2023_Dataset')\n",
        "dir_path='/content/temp_dir/CSE508_Winter2023_Dataset'\n",
        "zf = ZipFile(\"CSE508_Winter2023_Dataset3_preprocessed.zip\", \"w\")\n",
        "for dirname, subdirs, fles in os.walk(dir_path):\n",
        "    zf.write(dirname)\n",
        "    for filename in fles:\n",
        "        zf.write(os.path.join(dirname, filename))\n",
        "zf.close()\n",
        "files.download('/content/CSE508_Winter2023_Dataset3_preprocessed.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IwkvlO7CuCW_",
        "outputId": "eac9d459-e946-4b08-90ae-c712ed873868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f2f3e589-922c-49ed-aff6-d3bc8e94e4f2\", \"CSE508_Winter2023_Dataset3_preprocessed.zip\", 1324567)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_list=os.listdir('/content/temp_dir/CSE508_Winter2023_Dataset')\n",
        "dir_path='/content/temp_dir/CSE508_Winter2023_Dataset'"
      ],
      "metadata": {
        "id": "IRGLyS7D6MFt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with Binary Count"
      ],
      "metadata": {
        "id": "tnOLce895-8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to be modified\n",
        "\n",
        "class Tf_Idf_1:\n",
        "  def __init__(self,files_list,dir_path):\n",
        "    self.doc_names = []\n",
        "    self.tf_dict = dict()\n",
        "    self.idf_dict = dict()\n",
        "    self.tf_idf_mat = None\n",
        "    self.vocab = None\n",
        "    self.vocab_count = 0\n",
        "    self.docs_count = 0\n",
        "    self.query_vec = None\n",
        "    self.query_docs_score = dict()\n",
        "    self.docs = self.preprocess_docs(files_list,dir_path)\n",
        "\n",
        "\n",
        "  def preprocess_docs(self,files_list,dir_path):\n",
        "    docs = dict()\n",
        "    count = 0\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      text=fobj.read()\n",
        "      word_list = text.split(\",\")\n",
        "      docs[fl] = word_list\n",
        "      self.doc_names.append(fl)\n",
        "      count += 1\n",
        "      fobj.close()\n",
        "    self.docs_count = count\n",
        "    self.doc_names.sort()\n",
        "    return docs\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    vocab = set()\n",
        "    for w_list in self.docs.values():\n",
        "      vocab = vocab.union(set(w_list))\n",
        "    self.vocab = list(vocab)\n",
        "    self.vocab.sort()\n",
        "    self.vocab_count = len(self.vocab)\n",
        "\n",
        "\n",
        "  def compute_tf(self):\n",
        "    for t in self.vocab:\n",
        "      self.tf_dict[t] = dict()\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = 0\n",
        "    for d,w_list in self.docs.items():\n",
        "        for t in w_list:\n",
        "          self.tf_dict[t][d] = 1\n",
        "    \n",
        "\n",
        "  def compute_idf(self):\n",
        "    self.idf_dict = {t:0 for t in self.vocab}\n",
        "    for t in self.vocab:\n",
        "      for w_list in self.docs.values():\n",
        "        if t in w_list:\n",
        "          self.idf_dict[t] += 1\n",
        "    for t in self.vocab:\n",
        "      self.idf_dict[t] = math.log(float(self.docs_count/( self.idf_dict[t] + 1)))\n",
        "\n",
        "\n",
        "  def build_matrix(self):\n",
        "    self.build_vocab()\n",
        "    self.compute_tf()\n",
        "    self.compute_idf()\n",
        "    self.tf_idf_mat = [[0 for i in range(self.vocab_count)] for j in range(self.docs_count)]\n",
        "    for i in range(self.docs_count):\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        self.tf_idf_mat[i][j] = self.tf_dict[t][d] * self.idf_dict[t]\n",
        "\n",
        "\n",
        "  def preprocess_query(self,query):                              # function for preprocessing the input query string                   \n",
        "    query = query.lower()                                        # converting the texts of query string into lowercase\n",
        "    words = re.sub('[^\\w\\s]',' ',query)                          # removing non-alphanumeric and non-space characters from the query string\n",
        "    tokens = nltk.word_tokenize(words)                           # tokenizing the modified query string\n",
        "    new_tokens = [t for t in tokens if t not in stop_words]      # removing stopwords from the tokens obtained in last step\n",
        "    words_list = [t for t in new_tokens if t!=' ']               # removing all blank-space tokens\n",
        "    return words_list                                            # returning a list of filtered tokens\n",
        "\n",
        "\n",
        "  def vectorize_query(self,words):\n",
        "    self.query_vec = {self.vocab[i]:0 for i in range(self.vocab_count)}\n",
        "    for w in words:\n",
        "      if w in self.vocab:\n",
        "        self.query_vec[w] += 1\n",
        "\n",
        "\n",
        "\n",
        "  def get_query_docs_score(self):\n",
        "    for d in self.doc_names:\n",
        "      self.query_docs_score[d] = 0\n",
        "    for i in range(self.docs_count):\n",
        "      s = 0\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        if t in self.query_vec:\n",
        "          s += self.tf_idf_mat[i][j]\n",
        "      self.query_docs_score[d] = s\n",
        "\n",
        "\n",
        "\n",
        "  def process_query(self,query):\n",
        "    words = self.preprocess_query(query)\n",
        "    self.vectorize_query(words)\n",
        "    self.get_query_docs_score()\n",
        "    scores = sorted(self.query_docs_score.items(), key=lambda x:x[1], reverse=True)\n",
        "    for i in range(5):\n",
        "      print(scores[i][0],\" : \",scores[i][1])\n"
      ],
      "metadata": {
        "id": "W0aFYGZU6IRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj1 = Tf_Idf_1(files_list,dir_path)\n",
        "obj1.build_matrix()\n",
        "q = input(\"Enter the query : \")\n",
        "obj1.process_query(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMlr5DEq6IEm",
        "outputId": "8bada477-b9b3-4fdf-d984-6b2ff97e3496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query : The dynamic stability of Vehicles traversing ascending or, descending paths through the air and, the trajectory path is calculated.\n",
            "cranfield0244  :  821.7728352915254\n",
            "cranfield0344  :  767.5641243148101\n",
            "cranfield1313  :  753.8975398548773\n",
            "cranfield0798  :  745.2762640533031\n",
            "cranfield0792  :  735.3743658009739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl                                \n",
        "f1=open(\"TF_IDF_1.pkl\",'wb')\n",
        "pkl.dump(obj1,f1)                                 # store this class object obj as pkl file for future use\n",
        "f1.close()\n",
        "files.download('TF_IDF_1.pkl')                   # downloading the pickle file for storing locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Sf472kIb6Hze",
        "outputId": "9ffd43d4-0a3d-4b10-b967-f308768c9518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4e4e8039-1ab0-4ded-9cdc-a1dfc93c285f\", \"TF_IDF_1.pkl\", 211783633)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl   \n",
        "from google.colab import files    \n",
        "uploads=files.upload()                            \n",
        "f2=open(\"TF_IDF_1.pkl\",'rb')\n",
        "obj1=pkl.load(f2)                                  # load the pkl file stored earlier\n",
        "f2.close() \n",
        "q = input(\"Enter the query : \")\n",
        "obj1.process_query(q)\n",
        "\n",
        "# The results are presented in graphical form and it comprise of charts normal shock waves in argon-free air shock wave mach number and, both in the shock tube"
      ],
      "metadata": {
        "id": "hRY8fbYq6HkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with Raw count"
      ],
      "metadata": {
        "id": "7RUOAiMTd9Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tf_Idf_2:\n",
        "  def __init__(self,files_list,dir_path):\n",
        "    self.doc_names = []\n",
        "    self.tf_dict = dict()\n",
        "    self.idf_dict = dict()\n",
        "    self.tf_idf_mat = None\n",
        "    self.vocab = None\n",
        "    self.vocab_count = 0\n",
        "    self.docs_count = 0\n",
        "    self.query_vec = None\n",
        "    self.query_docs_score = dict()\n",
        "    self.docs = self.preprocess_docs(files_list,dir_path)\n",
        "\n",
        "\n",
        "  def preprocess_docs(self,files_list,dir_path):\n",
        "    docs = dict()\n",
        "    count = 0\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      text=fobj.read()\n",
        "      word_list = text.split(\",\")\n",
        "      docs[fl] = word_list\n",
        "      self.doc_names.append(fl)\n",
        "      count += 1\n",
        "      fobj.close()\n",
        "    self.docs_count = count\n",
        "    self.doc_names.sort()\n",
        "    return docs\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    vocab = set()\n",
        "    for w_list in self.docs.values():\n",
        "      vocab = vocab.union(set(w_list))\n",
        "    self.vocab = list(vocab)\n",
        "    self.vocab.sort()\n",
        "    self.vocab_count = len(self.vocab)\n",
        "\n",
        "\n",
        "  def compute_tf(self):\n",
        "    for t in self.vocab:\n",
        "      self.tf_dict[t] = dict()\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = 0\n",
        "    for d,w_list in self.docs.items():\n",
        "        for t in w_list:\n",
        "          self.tf_dict[t][d] += 1\n",
        "    \n",
        "\n",
        "  def compute_idf(self):\n",
        "    self.idf_dict = {t:0 for t in self.vocab}\n",
        "    for t in self.vocab:\n",
        "      for w_list in self.docs.values():\n",
        "        if t in w_list:\n",
        "          self.idf_dict[t] += 1\n",
        "    for t in self.vocab:\n",
        "      self.idf_dict[t] = math.log(float(self.docs_count/( self.idf_dict[t] + 1)))\n",
        "\n",
        "\n",
        "  def build_matrix(self):\n",
        "    self.build_vocab()\n",
        "    self.compute_tf()\n",
        "    self.compute_idf()\n",
        "    self.tf_idf_mat = [[0 for i in range(self.vocab_count)] for j in range(self.docs_count)]\n",
        "    for i in range(self.docs_count):\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        self.tf_idf_mat[i][j] = self.tf_dict[t][d] * self.idf_dict[t]\n",
        "\n",
        "\n",
        "  def preprocess_query(self,query):                              # function for preprocessing the input query string                   \n",
        "    query = query.lower()                                        # converting the texts of query string into lowercase\n",
        "    words = re.sub('[^\\w\\s]',' ',query)                          # removing non-alphanumeric and non-space characters from the query string\n",
        "    tokens = nltk.word_tokenize(words)                           # tokenizing the modified query string\n",
        "    new_tokens = [t for t in tokens if t not in stop_words]      # removing stopwords from the tokens obtained in last step\n",
        "    words_list = [t for t in new_tokens if t!=' ']               # removing all blank-space tokens\n",
        "    return words_list                                            # returning a list of filtered tokens\n",
        "\n",
        "\n",
        "  def vectorize_query(self,words):\n",
        "    self.query_vec = {self.vocab[i]:0 for i in range(self.vocab_count)}\n",
        "    for w in words:\n",
        "      if w in self.vocab:\n",
        "        self.query_vec[w] += 1\n",
        "\n",
        "\n",
        "\n",
        "  def get_query_docs_score(self):\n",
        "    for d in self.doc_names:\n",
        "      self.query_docs_score[d] = 0\n",
        "    for i in range(self.docs_count):\n",
        "      s = 0\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        if t in self.query_vec:\n",
        "          s += self.tf_idf_mat[i][j]\n",
        "      self.query_docs_score[d] = s\n",
        "\n",
        "\n",
        "\n",
        "  def process_query(self,query):\n",
        "    words = self.preprocess_query(query)\n",
        "    self.vectorize_query(words)\n",
        "    self.get_query_docs_score()\n",
        "    scores = sorted(self.query_docs_score.items(), key=lambda x:x[1], reverse=True)\n",
        "    for i in range(5):\n",
        "      print(scores[i][0],\" : \",scores[i][1])\n"
      ],
      "metadata": {
        "id": "1JWOtAwYr6vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = Tf_Idf_2(files_list,dir_path)\n",
        "obj.build_matrix()\n",
        "q = input(\"Enter the query : \")\n",
        "obj.process_query(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyB6GXP-ciwZ",
        "outputId": "f6365c48-6e2e-4bfd-f10e-49e78c00e830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query : The dynamic stability of Vehicles traversing ascending or, descending paths through the air and, the trajectory path is calculated.\n",
            "cranfield1313  :  1217.0463788252791\n",
            "cranfield0329  :  1142.2948257125734\n",
            "cranfield0244  :  1122.8897338910137\n",
            "cranfield0798  :  1064.1516265852347\n",
            "cranfield1040  :  998.1292773890652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl                                \n",
        "f1=open(\"TF_IDF_2.pkl\",'wb')\n",
        "pkl.dump(obj,f1)                                 # store this class object obj as pkl file for future use\n",
        "f1.close()\n",
        "files.download('TF_IDF_2.pkl')                   # downloading the pickle file for storing locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "D8QZtAOfx-dj",
        "outputId": "38f92453-daf2-40e9-ffbf-01aa5cba3035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_540789e2-343b-4850-b14d-b406499d5fac\", \"TF_IDF_2.pkl\", 211783633)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl   \n",
        "from google.colab import files    \n",
        "uploads=files.upload()                            \n",
        "f2=open(\"TF_IDF_2.pkl\",'rb')\n",
        "obj=pkl.load(f2)                                  # load the pkl file stored earlier\n",
        "f2.close() \n",
        "q = input(\"Enter the query : \")\n",
        "obj.process_query(q)\n",
        "\n",
        "# The results are presented in graphical form and it comprise of charts normal shock waves in argon-free air shock wave mach number and, both in the shock tube"
      ],
      "metadata": {
        "id": "gsIuPoHhxOZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with Term Frequency weighting scheme"
      ],
      "metadata": {
        "id": "WbsEySCr3TLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tf_Idf_3:\n",
        "  def __init__(self,files_list,dir_path):\n",
        "    self.doc_names = []\n",
        "    self.tf_dict = dict()\n",
        "    self.idf_dict = dict()\n",
        "    self.tf_idf_mat = None\n",
        "    self.vocab = None\n",
        "    self.vocab_count = 0\n",
        "    self.docs_count = 0\n",
        "    self.query_vec = None\n",
        "    self.query_docs_score = dict()\n",
        "    self.docs = self.preprocess_docs(files_list,dir_path)\n",
        "\n",
        "\n",
        "  def preprocess_docs(self,files_list,dir_path):\n",
        "    docs = dict()\n",
        "    count = 0\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      text=fobj.read()\n",
        "      word_list = text.split(\",\")\n",
        "      docs[fl] = word_list\n",
        "      self.doc_names.append(fl)\n",
        "      count += 1\n",
        "      fobj.close()\n",
        "    self.docs_count = count\n",
        "    self.doc_names.sort()\n",
        "    return docs\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    vocab = set()\n",
        "    for w_list in self.docs.values():\n",
        "      vocab = vocab.union(set(w_list))\n",
        "    self.vocab = list(vocab)\n",
        "    self.vocab.sort()\n",
        "    self.vocab_count = len(self.vocab)\n",
        "\n",
        "\n",
        "  def compute_tf(self):\n",
        "    for t in self.vocab:\n",
        "      self.tf_dict[t] = dict()\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = 0\n",
        "    for d,w_list in self.docs.items():\n",
        "        for t in w_list:\n",
        "          self.tf_dict[t][d] += 1\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = self.tf_dict[t][d] / len(self.docs[d])\n",
        "\n",
        "  def compute_idf(self):\n",
        "    self.idf_dict = {t:0 for t in self.vocab}\n",
        "    for t in self.vocab:\n",
        "      for w_list in self.docs.values():\n",
        "        if t in w_list:\n",
        "          self.idf_dict[t] += 1\n",
        "    for t in self.vocab:\n",
        "      self.idf_dict[t] = math.log(float(self.docs_count/( self.idf_dict[t] + 1)))\n",
        "\n",
        "\n",
        "  def build_matrix(self):\n",
        "    self.build_vocab()\n",
        "    self.compute_tf()\n",
        "    self.compute_idf()\n",
        "    self.tf_idf_mat = [[0 for i in range(self.vocab_count)] for j in range(self.docs_count)]\n",
        "    for i in range(self.docs_count):\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        self.tf_idf_mat[i][j] = self.tf_dict[t][d] * self.idf_dict[t]\n",
        "\n",
        "\n",
        "  def preprocess_query(self,query):                              # function for preprocessing the input query string                   \n",
        "    query = query.lower()                                        # converting the texts of query string into lowercase\n",
        "    words = re.sub('[^\\w\\s]',' ',query)                          # removing non-alphanumeric and non-space characters from the query string\n",
        "    tokens = nltk.word_tokenize(words)                           # tokenizing the modified query string\n",
        "    new_tokens = [t for t in tokens if t not in stop_words]      # removing stopwords from the tokens obtained in last step\n",
        "    words_list = [t for t in new_tokens if t!=' ']               # removing all blank-space tokens\n",
        "    return words_list                                            # returning a list of filtered tokens\n",
        "\n",
        "\n",
        "  def vectorize_query(self,words):\n",
        "    self.query_vec = {self.vocab[i]:0 for i in range(self.vocab_count)}\n",
        "    for w in words:\n",
        "      if w in self.vocab:\n",
        "        self.query_vec[w] += 1\n",
        "\n",
        "\n",
        "\n",
        "  def get_query_docs_score(self):\n",
        "    for d in self.doc_names:\n",
        "      self.query_docs_score[d] = 0\n",
        "    for i in range(self.docs_count):\n",
        "      s = 0\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        if t in self.query_vec:\n",
        "          s += self.tf_idf_mat[i][j]\n",
        "      self.query_docs_score[d] = s\n",
        "\n",
        "\n",
        "\n",
        "  def process_query(self,query):\n",
        "    words = self.preprocess_query(query)\n",
        "    self.vectorize_query(words)\n",
        "    self.get_query_docs_score()\n",
        "    scores = sorted(self.query_docs_score.items(), key=lambda x:x[1], reverse=True)\n",
        "    for i in range(5):\n",
        "      print(scores[i][0],\" : \",scores[i][1])\n"
      ],
      "metadata": {
        "id": "PJCyaG0W3b-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj3 = Tf_Idf_3(files_list,dir_path)\n",
        "obj3.build_matrix()\n",
        "q = input(\"Enter the query : \")\n",
        "obj3.process_query(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbg44jJG3cud",
        "outputId": "0ceb7ed9-73a5-463f-dc3b-b3994c7c9b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query :  The dynamic stability of Vehicles traversing ascending or, descending paths through the air and, the trajectory path is calculated. \n",
            "cranfield0471  :  6.145615226935241\n",
            "cranfield0995  :  6.145615226935241\n",
            "cranfield0718  :  4.9598345236312475\n",
            "cranfield1168  :  4.704507822251688\n",
            "cranfield0083  :  4.549459203324424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl                                \n",
        "f1=open(\"TF_IDF_3.pkl\",'wb')\n",
        "pkl.dump(obj3,f1)                                 # store this class object obj as pkl file for future use\n",
        "f1.close()\n",
        "files.download('TF_IDF_3.pkl')                   # downloading the pickle file for storing locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "brl-HeQG3dPZ",
        "outputId": "b7e24b51-e962-4ccf-8b68-a76c35467b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d0dc0ac6-d595-4aa8-979c-eda6d02f1a2a\", \"TF_IDF_3.pkl\", 306837083)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl   \n",
        "from google.colab import files    \n",
        "uploads=files.upload()                            \n",
        "f2=open(\"TF_IDF_3.pkl\",'rb')\n",
        "obj3=pkl.load(f2)                                  # load the pkl file stored earlier\n",
        "f2.close() \n",
        "q = input(\"Enter the query : \")\n",
        "obj3.process_query(q)\n",
        "\n",
        "# The results are presented in graphical form and it comprise of charts normal shock waves in argon-free air shock wave mach number and, both in the shock tube"
      ],
      "metadata": {
        "id": "uiuISCL53fOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with Log Normalization weighting scheme"
      ],
      "metadata": {
        "id": "1m1tviWk2y8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tf_Idf_4:\n",
        "  def __init__(self,files_list,dir_path):\n",
        "    self.doc_names = []\n",
        "    self.tf_dict = dict()\n",
        "    self.idf_dict = dict()\n",
        "    self.tf_idf_mat = None\n",
        "    self.vocab = None\n",
        "    self.vocab_count = 0\n",
        "    self.docs_count = 0\n",
        "    self.query_vec = None\n",
        "    self.query_docs_score = dict()\n",
        "    self.docs = self.preprocess_docs(files_list,dir_path)\n",
        "\n",
        "\n",
        "  def preprocess_docs(self,files_list,dir_path):\n",
        "    docs = dict()\n",
        "    count = 0\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      text=fobj.read()\n",
        "      word_list = text.split(\",\")\n",
        "      docs[fl] = word_list\n",
        "      self.doc_names.append(fl)\n",
        "      count += 1\n",
        "      fobj.close()\n",
        "    self.docs_count = count\n",
        "    self.doc_names.sort()\n",
        "    return docs\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    vocab = set()\n",
        "    for w_list in self.docs.values():\n",
        "      vocab = vocab.union(set(w_list))\n",
        "    self.vocab = list(vocab)\n",
        "    self.vocab.sort()\n",
        "    self.vocab_count = len(self.vocab)\n",
        "\n",
        "\n",
        "  def compute_tf(self):\n",
        "    for t in self.vocab:\n",
        "      self.tf_dict[t] = dict()\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = 0\n",
        "    for d,w_list in self.docs.items():\n",
        "        for t in w_list:\n",
        "          self.tf_dict[t][d] += 1\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = math.log(1 + self.tf_dict[t][d])\n",
        "\n",
        "  def compute_idf(self):\n",
        "    self.idf_dict = {t:0 for t in self.vocab}\n",
        "    for t in self.vocab:\n",
        "      for w_list in self.docs.values():\n",
        "        if t in w_list:\n",
        "          self.idf_dict[t] += 1\n",
        "    for t in self.vocab:\n",
        "      self.idf_dict[t] = math.log(float(self.docs_count/( self.idf_dict[t] + 1)))\n",
        "\n",
        "\n",
        "  def build_matrix(self):\n",
        "    self.build_vocab()\n",
        "    self.compute_tf()\n",
        "    self.compute_idf()\n",
        "    self.tf_idf_mat = [[0 for i in range(self.vocab_count)] for j in range(self.docs_count)]\n",
        "    for i in range(self.docs_count):\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        self.tf_idf_mat[i][j] = self.tf_dict[t][d] * self.idf_dict[t]\n",
        "\n",
        "\n",
        "  def preprocess_query(self,query):                              # function for preprocessing the input query string                   \n",
        "    query = query.lower()                                        # converting the texts of query string into lowercase\n",
        "    words = re.sub('[^\\w\\s]',' ',query)                          # removing non-alphanumeric and non-space characters from the query string\n",
        "    tokens = nltk.word_tokenize(words)                           # tokenizing the modified query string\n",
        "    new_tokens = [t for t in tokens if t not in stop_words]      # removing stopwords from the tokens obtained in last step\n",
        "    words_list = [t for t in new_tokens if t!=' ']               # removing all blank-space tokens\n",
        "    return words_list                                            # returning a list of filtered tokens\n",
        "\n",
        "\n",
        "  def vectorize_query(self,words):\n",
        "    self.query_vec = {self.vocab[i]:0 for i in range(self.vocab_count)}\n",
        "    for w in words:\n",
        "      if w in self.vocab:\n",
        "        self.query_vec[w] += 1\n",
        "\n",
        "\n",
        "\n",
        "  def get_query_docs_score(self):\n",
        "    for d in self.doc_names:\n",
        "      self.query_docs_score[d] = 0\n",
        "    for i in range(self.docs_count):\n",
        "      s = 0\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        if t in self.query_vec:\n",
        "          s += self.tf_idf_mat[i][j]\n",
        "      self.query_docs_score[d] = s\n",
        "\n",
        "\n",
        "\n",
        "  def process_query(self,query):\n",
        "    words = self.preprocess_query(query)\n",
        "    self.vectorize_query(words)\n",
        "    self.get_query_docs_score()\n",
        "    scores = sorted(self.query_docs_score.items(), key=lambda x:x[1], reverse=True)\n",
        "    for i in range(5):\n",
        "      print(scores[i][0],\" : \",scores[i][1])\n"
      ],
      "metadata": {
        "id": "9dnQStONxJtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj4 = Tf_Idf_4(files_list,dir_path)\n",
        "obj4.build_matrix()\n",
        "q = input(\"Enter the query : \")\n",
        "obj4.process_query(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH0kjzzu1HBd",
        "outputId": "59c01683-f15f-4816-b755-1c9a109e0cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query : The dynamic stability of Vehicles traversing ascending or, descending paths through the air and, the trajectory path is calculated.\n",
            "cranfield1313  :  655.825391422564\n",
            "cranfield0244  :  653.7301962297629\n",
            "cranfield0798  :  617.2127232284711\n",
            "cranfield0329  :  603.6769625624271\n",
            "cranfield0344  :  588.0843232452198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl                                \n",
        "f1=open(\"TF_IDF_4.pkl\",'wb')\n",
        "pkl.dump(obj4,f1)                                 # store this class object obj as pkl file for future use\n",
        "f1.close()\n",
        "files.download('TF_IDF_4.pkl')                   # downloading the pickle file for storing locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2hpSzfOR1Xn1",
        "outputId": "5315d0e4-226c-46cf-fb1b-d43b16133479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8309abef-2f9d-4263-90ce-3ba4e9bf3a40\", \"TF_IDF_4.pkl\", 306837083)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl   \n",
        "from google.colab import files    \n",
        "uploads=files.upload()                            \n",
        "f2=open(\"TF_IDF_4.pkl\",'rb')\n",
        "obj4=pkl.load(f2)                                  # load the pkl file stored earlier\n",
        "f2.close() \n",
        "q = input(\"Enter the query : \")\n",
        "obj4.process_query(q)\n",
        "\n",
        "# The results are presented in graphical form and it comprise of charts normal shock waves in argon-free air shock wave mach number and, both in the shock tube"
      ],
      "metadata": {
        "id": "8QcLZ4Jr14l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with Double Normalization weighting scheme"
      ],
      "metadata": {
        "id": "4H-vzwUL7DeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to be modified\n",
        "\n",
        "class Tf_Idf_5:\n",
        "  def __init__(self,files_list,dir_path):\n",
        "    self.doc_names = []\n",
        "    self.tf_dict = dict()\n",
        "    self.idf_dict = dict()\n",
        "    self.tf_idf_mat = None\n",
        "    self.vocab = None\n",
        "    self.vocab_count = 0\n",
        "    self.docs_count = 0\n",
        "    self.query_vec = None\n",
        "    self.query_docs_score = dict()\n",
        "    self.docs = self.preprocess_docs(files_list,dir_path)\n",
        "\n",
        "\n",
        "  def preprocess_docs(self,files_list,dir_path):\n",
        "    docs = dict()\n",
        "    count = 0\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      text=fobj.read()\n",
        "      word_list = text.split(\",\")\n",
        "      docs[fl] = word_list\n",
        "      self.doc_names.append(fl)\n",
        "      count += 1\n",
        "      fobj.close()\n",
        "    self.docs_count = count\n",
        "    self.doc_names.sort()\n",
        "    return docs\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    vocab = set()\n",
        "    for w_list in self.docs.values():\n",
        "      vocab = vocab.union(set(w_list))\n",
        "    self.vocab = list(vocab)\n",
        "    self.vocab.sort()\n",
        "    self.vocab_count = len(self.vocab)\n",
        "\n",
        "\n",
        "  def compute_tf(self):\n",
        "    for t in self.vocab:\n",
        "      self.tf_dict[t] = dict()\n",
        "    for t in self.vocab:\n",
        "      for d in self.doc_names:\n",
        "        self.tf_dict[t][d] = 0\n",
        "    for d,w_list in self.docs.items():\n",
        "        for t in w_list:\n",
        "          self.tf_dict[t][d] += 1\n",
        "    \n",
        "\n",
        "  def compute_idf(self):\n",
        "    self.idf_dict = {t:0 for t in self.vocab}\n",
        "    for t in self.vocab:\n",
        "      for w_list in self.docs.values():\n",
        "        if t in w_list:\n",
        "          self.idf_dict[t] += 1\n",
        "    for t in self.vocab:\n",
        "      self.idf_dict[t] = math.log(float(self.docs_count/( self.idf_dict[t] + 1)))\n",
        "\n",
        "\n",
        "  def build_matrix(self):\n",
        "    self.build_vocab()\n",
        "    self.compute_tf()\n",
        "    self.compute_idf()\n",
        "    self.tf_idf_mat = [[0 for i in range(self.vocab_count)] for j in range(self.docs_count)]\n",
        "    for i in range(self.docs_count):\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        self.tf_idf_mat[i][j] = self.tf_dict[t][d] * self.idf_dict[t][d]\n",
        "\n",
        "\n",
        "  def preprocess_query(self,query):                              # function for preprocessing the input query string                   \n",
        "    query = query.lower()                                        # converting the texts of query string into lowercase\n",
        "    words = re.sub('[^\\w\\s]',' ',query)                          # removing non-alphanumeric and non-space characters from the query string\n",
        "    tokens = nltk.word_tokenize(words)                           # tokenizing the modified query string\n",
        "    new_tokens = [t for t in tokens if t not in stop_words]      # removing stopwords from the tokens obtained in last step\n",
        "    words_list = [t for t in new_tokens if t!=' ']               # removing all blank-space tokens\n",
        "    return words_list                                            # returning a list of filtered tokens\n",
        "\n",
        "\n",
        "  def vectorize_query(self,words):\n",
        "    self.query_vec = {self.vocab[i]:0 for i in range(self.vocab_count)}\n",
        "    for w in words:\n",
        "      if w in self.vocab:\n",
        "        self.query_vec[w] += 1\n",
        "\n",
        "\n",
        "\n",
        "  def get_query_docs_score(self):\n",
        "    for d in self.doc_names:\n",
        "      self.query_docs_score[d] = 0\n",
        "    for i in range(self.docs_count):\n",
        "      s = 0\n",
        "      for j in range(self.vocab_count):\n",
        "        t = self.vocab[j]\n",
        "        d = self.doc_names[i]\n",
        "        if t in self.query_vec:\n",
        "          s += self.tf_idf_mat[i][j]\n",
        "      self.query_docs_score[d] = s\n",
        "\n",
        "\n",
        "\n",
        "  def process_query(self,query):\n",
        "    words = self.preprocess_query(query)\n",
        "    self.vectorize_query(words)\n",
        "    self.get_query_docs_score()\n",
        "    scores = sorted(self.query_docs_score.items(), key=lambda x:x[1], reverse=True)\n",
        "    for i in range(5):\n",
        "      print(scores[i][0],\" : \",scores[i][1])\n"
      ],
      "metadata": {
        "id": "giBn0GPx7isy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj5 = Tf_Idf_5(files_list,dir_path)\n",
        "obj5.build_matrix()\n",
        "q = input(\"Enter the query : \")\n",
        "obj5.process_query(q)"
      ],
      "metadata": {
        "id": "flLjDjNc7cUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl                                \n",
        "f1=open(\"TF_IDF_5.pkl\",'wb')\n",
        "pkl.dump(obj5,f1)                                 # store this class object obj as pkl file for future use\n",
        "f1.close()\n",
        "files.download('TF_IDF_5.pkl')                   # downloading the pickle file for storing locally"
      ],
      "metadata": {
        "id": "XUL14aGp7XjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl   \n",
        "from google.colab import files    \n",
        "uploads=files.upload()                            \n",
        "f2=open(\"TF_IDF_5.pkl\",'rb')\n",
        "obj5=pkl.load(f2)                                  # load the pkl file stored earlier\n",
        "f2.close() \n",
        "q = input(\"Enter the query : \")\n",
        "obj5.process_query(q)\n",
        "\n",
        "# The results are presented in graphical form and it comprise of charts normal shock waves in argon-free air shock wave mach number and, both in the shock tube"
      ],
      "metadata": {
        "id": "RZ24pL3T7CVV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}